{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3.1 - Regression\n",
    "\n",
    "In this tutorial, we describe the basics of solving a regression-based machine learning problem on top of behavioral data, and give you a comparative study of some of the current most popular algorithms.\n",
    "\n",
    "**Expected Tasks**\n",
    "\n",
    "- Follow the examples on regression pipelines.\n",
    "- Complete a short use case on classification.  \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Load and understand the data at hand. \n",
    "- Extract meaningful features from the raw data. \n",
    "- Split data into training and test sets.\n",
    "- Scale features before feeding them into an algorithm. \n",
    "- Select and train a regressors according to the domain. \n",
    "- Make predictions and evaluate accuracy. \n",
    "\n",
    "**Notes**\n",
    "\n",
    "Regressors are demonstrated in this notebook using small code recipes in Python and ScikitLearn.\n",
    "\n",
    "More information on regression algorithms supported by ScikitLearn are listed on the page [Supervised learning](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supporting packages\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics, preprocessing, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Data generators help us create data with **different distributions** and profiles to experiment on, while testing algorithms. Specifically, data generators can help us generate case specific data and, then, test the algorithm. \n",
    "\n",
    "Few **reasons** why you need generated data:\n",
    "- Can your models handle noisy labels?\n",
    "- Does removing redundant features improve your model’s performance?\n",
    "- How does your model behave with redundant features, noise and imbalance?\n",
    "\n",
    "Finding a real dataset meeting such combination of criterias with known levels will be very difficult. As a result, you might first work with synthetic datasets that can replicate certain circumstances present in **real-world datasets**.\n",
    "\n",
    "More information on synthetic data generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=100, n_features=10, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y - y.min()) / (y.max() - y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.min(), y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "Training and test sets are used to **estimate the performance of machine learning algorithms** when they are used to make predictions on **data not used to train the model**. In this way, we will be able to compare the performance of machine learning algorithms for the predictive problem we are considering. \n",
    "\n",
    "This split is the simplet to use and interpret. However, for instance, when you have a small dataset, there are other strategies that can be used. We will delve into more advanced procedures next week (e.g., cross-validation procedures). \n",
    "\n",
    "More information on train-test sets generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "Models learn a mapping from input variables to an output variable. However, input variables may have **different units** (e.g. feet, kilometers, and hours) that, in turn, may mean the variables have **different scales**. By extension, differences in the scales across input variables may increase the **difficulty of the problem being modeled** (e.g., a variable with large values can result in a model that learns large weight values, a model with large weight values is often **unstable**). Consequently,  it may suffer from poor performance during learning and sensitivity to input values resulting in **higher generalization error**.\n",
    "\n",
    "There are two types of scaling of your data we will cover in this tutorial: \n",
    "- Normalization\n",
    "- Standardization \n",
    "\n",
    "More information on data scaling can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/preprocessing.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is a rescaling of the data from the original range so that **all values are within the range of 0 and 1**. Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. \n",
    "\n",
    "Good practice usage with the MinMaxScaler and other scaling techniques is as follows:\n",
    "\n",
    "- Fit the scaler using available **training** data. This is done by calling the fit() function.\n",
    "- Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function.\n",
    "- Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that **the mean of observed values is 0 and the standard deviation is 1**. In other words, we are subtracting the mean value or centering the data. Standardization can be useful or even crucial in some machine learning algorithms, when your data has input values with differing scales.\n",
    "\n",
    "Standardization assumes that your observations **fit a Gaussian distribution (bell curve)** with a well behaved mean and standard deviation. You can still standardize your data if this expectation is not met, but you may not get reliable results. Standardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train, axis=0), np.var(X_train, axis=0), np.mean(X_test, axis=0), np.var(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "\n",
    "Linear models are a class of models that are widely used in practice and have been studied extensively in the last few decades, with roots going back over a hundred years ● Linear models make a prediction using a linear function of the input features:\n",
    "\n",
    "$ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b$\n",
    "\n",
    "Here, $x[0]$ to $x[p]$ denote the features (in this example, the number of features is p+1) of a single data point, w and b are parameters of the model that are learned, and $ŷ$ is the prediction the model makes. For a dataset with a single feature, this is:\n",
    "\n",
    "$ŷ = w[0] * x[0] + b$\n",
    "\n",
    "Linear models for regression can be characterized as regression models for which the prediction is a line for a single feature, a plane when using two features, or a hyperplane in higher dimensions. If you compare the predictions made by the straight line with those made by the KNeighborsRegressor, using a straight line to make predictions seems very restrictive. For datasets with many features, linear models can be very powerful. In particular, if you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. \n",
    "\n",
    "More information on data scaling Linear Regression can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models with statsmodels\n",
    "\n",
    "Statsmodels is a Python module which provides various functions for estimating different statistical models and performing statistical tests. \n",
    "\n",
    "In this tutorial, we will focus on:\n",
    "- Linear models\n",
    "- Generalized linear models\n",
    "    - Logistic regression\n",
    "    - Poisson regression\n",
    "\n",
    "More information on the statsmodels library can be found in the [official documentation](https://www.statsmodels.org/devel/gettingstarted.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression models\n",
    "\n",
    "Statsmodels provides linear models with independently and identically distributed errors, and for errors with heteroscedasticity or autocorrelation. This module allows estimation by ordinary least squares (OLS), weighted least squares (WLS), generalized least squares (GLS), and feasible generalized least squares with autocorrelated AR(p) errors.\n",
    "\n",
    "We will focus on the basic one, namely ordinary least squares (OLS). \n",
    "\n",
    "More information on the linear models in statsmodels can be found [here](https://www.statsmodels.org/devel/regression.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, ‘Iterations‘ refer to the number of times the model iterates over the data, trying to optimise the model. By default, the maximum number of iterations performed is 5, after which the optimisation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of some of the terms in the summary table:\n",
    "\n",
    "- **The dependent variable**: y\n",
    "- **Model** and **Method**: The type of model and method that was fitted (OLS, Least Squares)\n",
    "- **Nb observations**: The number of datapoints (80 patients)\n",
    "- **R-squared**: The fraction of explained variance\n",
    "- **Log-Likelihood**: the natural logarithm of the Maximum Likelihood Estimation(MLE) function. MLE is the optimisation process of finding the set of parameters which result in best fit.\n",
    "- A list of predictors and, fo each predictor: \n",
    "    - coefficient\n",
    "    - standard error of the coefficients\n",
    "    - t is a measurement of the precision with which the coefficient was measured.\n",
    "    - P>|t| uses the t statistic to produce the p value, a measurement of how likely your coefficient is measured through our model by chance. For instance, a p value of 0.185 for x1 is saying there is a 18.5% chance x1 has no affect on the dependent variable y. \n",
    "    - [0.025 and 0.975] are both measurements of values of our coefficients within 95% of our data, or within two standard deviations. Outside of these values can generally be considered outliers.\n",
    "\n",
    "Finally, you could compute the predictions on the test set by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalizes Linear Models\n",
    "\n",
    "The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value [from Wikipedia]\n",
    "(https://en.wikipedia.org/wiki/Generalized_linear_model). \n",
    "\n",
    "- Logistic regression\n",
    "- Poisson regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = sm.GLM(y_train, X_train, family=sm.families.Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pss_reg = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pss_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other link functions can be found [here](https://www.statsmodels.org/devel/glm.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Decision trees are widely adopted models for classification and regression tasks, with minimal differences in their usage. They learn a **hierarchy of if/else questions**, leading to a decision. Learning a decision tree means learning the sequence of if/else questions that gets us to the true answer quickly. In the machine learning setting, **these questions are called tests** (not to be confused with the test set). Usually data does not come in the form of binary yes/no features, but is instead represented as continuous features. The tests that are used on continuous data are of the form “Is feature i larger than value a?”. A prediction on a new data point is made by checking which region of the partition of the feature space the point lies in, and then **predicting the target** based on the data samples in the region.\n",
    "\n",
    "Pros and cons decision trees:\n",
    "\n",
    "- The parameters that control model complexity are the **pre-pruning parameters** that stop the building of the tree before it is fully developed. \n",
    "- The resulting model can easily be **visualized and understood by nonexperts**, and the algorithms are completely invariant to scaling. \n",
    "- Features are processed separately, and splits do not depend on scaling, **no preprocessing** like normalization or standardization of features is needed. \n",
    "- In particular, decision trees work well when you have features that are on **completely different scales**, or a mix of binary and continuous features.\n",
    "- The main downside of decision trees is that even with the use of pre-pruning, they **tend to overfit** and provide poor generalization performance\n",
    "\n",
    "More information on data scaling decision trees for regression can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Ensembles are methods that **combine multiple machine learning models** to create more powerful models. Two ensemble models that have proven to be effective use decision trees as their building blocks: Random Forests and Gradient Boosting. Here, we will cover only **Random Forests**. \n",
    "\n",
    "The main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data. If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results.\n",
    "\n",
    "Pros and cons ensemble methods:\n",
    "- They are very **powerful**, often work well without heavy tuning, and do not require scaling of the data. \n",
    "- It is basically **difficult to interpret** tens or hundreds of trees in detail. \n",
    "- Trees in random forests tend to be **deeper than decision trees** (because of the use of feature subsets). \n",
    "- While building random forests on large datasets might be somewhat time consuming, it **can be parallelized** across multiple CPU. \n",
    "- Random forests do **not** tend to perform well on **very high dimensional, sparse data**, such as text data. \n",
    "- Random forests usually work well even on **very large datasets**. \n",
    "- They **require more memory and are slower** to train and to predict than linear models. \n",
    "- The important parameters to adjust are **n_estimators**, **max_features**, and **pre-pruning** options like **max_depth**.\n",
    "\n",
    "More information on data scaling ensembles can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/ensemble.html). You can play with other ensembles listed in this page (e.g., Gradient Boosting classifiers).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators=10, random_state= 0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (k-NN)\n",
    "\n",
    "The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset; to make a prediction for a new data point, the algorithm **finds the closest data points in the training dataset**, that are its “nearest neighbors”. In its simplest version, the k-NN algorithm only considers exactly one nearest neighbor, the closest training data point to the point we want to predict for. \n",
    "\n",
    "Instead of considering only the closest neighbor, we can also consider **an arbitrary number k of neighbors**. When considering more than one neighbor, target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set. \n",
    "\n",
    "Pros and cons k-nearest neighbors:\n",
    "\n",
    "- Choosing the **number of neighbors** is challenging; in practice, using a small number often works well, but you should certainly adjust this parameter.\n",
    "- Choosing the **right distance measure** is dependant from the underlying data; by default, Euclidean distance is used, which works well in many settings.\n",
    "- The model is **very easy to understand**, and often gives reasonable performance without a lot of adjustments; it is a good baseline method.\n",
    "- Building the nearest neighbors model is usually very fast. \n",
    "- When your training set is very large (either in number of features or in number of samples) **prediction can be slow**.\n",
    "- When using the k-NN algorithm, it’s important to preprocess your data.\n",
    "- This approach often does **not perform well on datasets with many features** (hundreds or more) or that are **sparse**. \n",
    "\n",
    "More information on data scaling k-NN can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machines\n",
    "\n",
    "Kernelized support vector machines (often just referred to as SVMs) are an extension that allows for more complex models that are not defined simply by hyperplanes in the input space. There are support vector machines for classification and regression. \n",
    "\n",
    "Pros and cons SVM:\n",
    "\n",
    "- SVMs allow for **complex decision boundaries**, even if the data has only a few features.\n",
    "- They work well on **low-dimensional and high-dimensional data** (i.e., few and many features). \n",
    "- They do **not** scale very well with **the number of samples**.\n",
    "- SVMs require **careful preprocessing** of the data and tuning of the parameters.\n",
    "- SVM models are **hard to inspect**; it can be difficult to understand why a particular prediction was made. \n",
    "- The important parameters in kernel SVMs are the regularization parameter **C**, the choice of the **kernel**, and the kernel-specific parameters. \n",
    "\n",
    "More information on data scaling Kernelized support vector machines can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "lsvr = SVR()\n",
    "lsvr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction\n",
    "\n",
    "Once you have trained your regressor, you may want to the values for X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Each regressor has a built-in method for computing the $\\mathcal{R}^2$, i..e, the proportion of the variance in the dependent variable that is predictable from the independent variable(s), on data and labels. The higher, the better. We will delve into evaluation metrics extensively next week. \n",
    "\n",
    "In other words, R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n",
    "\n",
    "$R^2 = \\text{Explained variation} / \\text{Total variation}$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on training set: {:.3f}\" .format(forest.score(X_train, y_train))) \n",
    "print(\"R2 on test set: {:.3f}\" .format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn: Student's Performance\n",
    "\n",
    "**Reference**: [Students Performance Dataset](https://archive.ics.uci.edu/ml/datasets/student+performance). \n",
    "\n",
    "P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n",
    "\n",
    "This data approach student achievement in **secondary education of two Portuguese schools**. The data attributes include student grades, demographic, social and school related features) and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics (mat) and Portuguese language (por). Here, we will use only the first one, but you could replicate the analysis also on the second one.  \n",
    "\n",
    "**Important Note**: the target attribute G3 has a strong correlation with attributes G2 and G1. This occurs because G3 is the final year grade (issued at the 3rd period), while G1 and G2 correspond to the 1st and 2nd period grades. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful (see paper source for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/student-mat.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "1. school - student's school (binary: \"GP\" - Gabriel Pereira or \"MS\" - Mousinho da Silveira)\n",
    "2. sex - student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "3. age - student's age (numeric: from 15 to 22)\n",
    "4. address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "5. famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "6. Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "7. Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "8. Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "9. Mjob - mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "10. Fjob - father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "11. reason - reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n",
    "12. guardian - student's guardian (nominal: \"mother\", \"father\" or \"other\")\n",
    "13. traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "14. studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "15. failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "16. schoolsup - extra educational support (binary: yes or no)\n",
    "17. famsup - family educational support (binary: yes or no)\n",
    "18. paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "19. activities - extra-curricular activities (binary: yes or no)\n",
    "20. nursery - attended nursery school (binary: yes or no)\n",
    "21. higher - wants to take higher education (binary: yes or no)\n",
    "22. internet - Internet access at home (binary: yes or no)\n",
    "23. romantic - with a romantic relationship (binary: yes or no)\n",
    "24. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "25. freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "26. goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "27. Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "28. Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "29. health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "30. absences - number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "What are the **targets**?\n",
    "\n",
    "31. G1 - first period grade (numeric: from 0 to 20)\n",
    "31. G2 - second period grade (numeric: from 0 to 20)\n",
    "32. G3 - final grade (numeric: from 0 to 20, output target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.1: Grade distribution\n",
    "- Plot a distribution of the final grade \"G3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.2: Preparing the data\n",
    "\n",
    "Separate features in **X** and the target variable in **y**.\n",
    "\n",
    "- Pick the column \"G3\" and copy its values into **y**. \n",
    "- Pick up to five **numerical features** and copy them into **X**.\n",
    "\n",
    "Which numerical features have you selected? Why? \n",
    "\n",
    "**Note**: We wil see how we could include categorical features in the next tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y - y.min()) / (y.max() - y.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test sets, namely **X_train**, **X_test**, **y_train**, **y_test**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data in **X_train** and **X_test**.\n",
    "\n",
    "- Is scaling important in the context of this dataset? How this decision might influence the other decisions along the pipeline?\n",
    "- Which scaling approach would you select? Why?\n",
    "- Is your decision at this point influenced by the model you will choose to train? You might need to come back later at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.3: Train a linear model\n",
    "\n",
    "Based on the characteristics of the dataset, choose a **regression model** among those we have presented and train it.\n",
    "\n",
    "You can use either statsmodels or scikit-learn. \n",
    "\n",
    "- Which model have you selected? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.4 Interpret your model \n",
    "\n",
    "Inspect the internal mechanics of your model and try to interpret them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.5: Make predictions\n",
    "\n",
    "- Compute predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.6: Calculate effectiveness\n",
    "\n",
    "- Compute the coefficient of determination $\\mathcal{R}^2$ on the test set. \n",
    "\n",
    "**Hint**: If you used the statsmodels package along the pipeline, please use the [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) function from scikit-learn, to compute the score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How you go further during or after the tutorial (based on your progress)\n",
    " \n",
    "During (if you have still time) or after the tutorial, you could go further by:\n",
    "\n",
    "- Run your pipeline for other models.\n",
    "- Compare the $R^2$ score across models. \n",
    "- Consider other subsets of the features.  \n",
    "- Investigate the impact of each feature in the predictions. \n",
    "- Replicate this study for the other students' class (./Data/student_pr.csv) and(or for the mid-term grades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we have presented the basics of data generation, train-test split, scaling with the scikit-learn library. Then, we have introduced a range of regression models and how they can be implemented, trained, and tested in the same library (with linear models we also presented the statsmodels library). Finally, we have investigated how to operationalize a regression pipeline with an example dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
