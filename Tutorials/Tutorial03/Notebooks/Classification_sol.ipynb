{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3.2 - Classification\n",
    "\n",
    "In this tutorial, we describe the basics of solving a classification-based machine learning problem on top of behavioral data, and give you a comparative study of some of the current most popular algorithms.\n",
    "\n",
    "**Expected Tasks**\n",
    "\n",
    "- Follow the examples on classification pipelines.\n",
    "- Complete a range of exercies on classification.  \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Load and understand the data at hand. \n",
    "- Extract meaningful features from the raw data. \n",
    "- Split data into training and test sets.\n",
    "- Scale features before feeding them into an algorithm. \n",
    "- Select and train a classifier according to the domain. \n",
    "- Make predictions and evaluate accuracy. \n",
    "\n",
    "**Notes**\n",
    "\n",
    "Classifiers are demonstrated in this notebook using small code recipes in Python and ScikitLearn.\n",
    "\n",
    "More information on classification algorithms supported by ScikitLearn are listed on the page [Supervised learning](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supporting packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, metrics, preprocessing, tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We already discussed why generating datasets for different purposes, such as regression and classification. Now, we can see how to to this for classification.\n",
    "\n",
    "More information on synthetic data generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=100, n_features=10, n_redundant=0, n_informative=10, class_sep=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "\n",
    "We use the same train-test split procedure covered in the regression tutorial.  \n",
    "\n",
    "More information on train-test sets generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "We use the same scaling procedures covered in the classification tutorial.\n",
    "\n",
    "More information on data scaling can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/preprocessing.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is a rescaling of the data from the original range so that **all values are within the range of 0 and 1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that **the mean of observed values is 0 and the standard deviation is 1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train, axis=0), np.var(X_train, axis=0), np.mean(X_test, axis=0), np.var(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "\n",
    "Linear models are extensively used also for classification. \n",
    "\n",
    "In case of a binary classification, a prediction is made using the following formula: \n",
    "\n",
    "$Å· = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0$\n",
    "\n",
    "The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. For linear models for classification, the decision boundary is a linear function of the input, that is a binary linear classifier is a classifier that separates two classes using a line, a plane, or a hyperplane. \n",
    "\n",
    "The linear classification algorithms we will cover in this tutorial are:\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Machine\n",
    "\n",
    "A common technique to extend a binary to a multiclass classification algorithm is the one-vs.-rest approach. In one-vs.-rest approach, a binary model is learned for each class that tries to separate that class from all of the other classes. To make a prediction, all binary classifiers are run on a test point. The classifier with the highest score on its single class prevails. \n",
    "\n",
    "A few considerations on linear models:\n",
    "- One of the main parameters of linear models is the regularization parameter, called **C** in LinearSVC and LogisticRegression. Tuning this parameter is quite important.\n",
    "- Another decision to make would be whether you want to use **L1 regularization** or **L2 regularization**. If you assume that only a few of your features are actually important, you could use L1. Otherwise, you should default to L2. L1 can also be useful if interpretability of the model is important. \n",
    "- These models should scale to **very large datasets** and work well with **sparse data**. In that case, you could investigate the solver='sag' in LogisticRegression and Ridge, which is faster than the default on large datasets. In lower-dimensional spaces, other models might yield better generalization performance.\n",
    "- One strength of linear models is that they make it **relatively easy to understand** how a prediction is made, using the formulas we saw earlier. \n",
    "\n",
    "More information on data scaling Logistic Regression can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).  \n",
    "\n",
    "More information on data scaling Linear SVM can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state= 0)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(random_state= 0)\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "All that was said is similarly true for decision trees for classification, as implemented in DecisionTreeClassifier. The only difference is how to compute the predicted target in the region: in this case, in general, it is picked the most represented target value in the region. \n",
    "\n",
    "More information on data scaling Decision Trees can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/tree.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods: Random Forests\n",
    "\n",
    "The procedure is the same followed for the regression case, except for the fact that target is predicted by the majorityof the targets associated of the forest. Pros and cons are the same as for regression.   \n",
    "\n",
    "More information on data scaling Ensembles can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/ensemble.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=10, random_state= 0)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (k-NN)\n",
    "\n",
    "The procedure is the same followed for the classification case, except for how to determine the target predicted value. For classification, we use voting to assign a label: for each test point, we count how many neighbors belong to each of the classes. We then assign **the class that is more frequent**: in other words, the majority class among the k-nearest neighbors. Pros and cons are the same as for classification.\n",
    "\n",
    "More information on data scaling k-NN can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machines\n",
    "\n",
    "Similar considerations and logic with respect to the regression variant. \n",
    "\n",
    "More information on data scaling Kernelized support vector machines can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/svm.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svmsvc = svm.SVC(max_iter=1000, random_state=0)\n",
    "svmsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction\n",
    "\n",
    "Once you have trained your classifier, you may want to:\n",
    "\n",
    "- predict class for X;\n",
    "- predict class probabilities for X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Each classifier has a built-in method for computing the mean accuracy on data and labels. We will delve into evaluation metrics extensively next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\" .format(forest.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\" .format(forest.score(X_test, y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Your Turn: Students' Grade Level Prediction\n",
    "\n",
    "**Reference**: [Kalboard 360 Dataset](https://www.kaggle.com/c/student-academic-performance). \n",
    "\n",
    "This is an educational data set which is collected from **learning management system** (LMS) called Kalboard 360. Kalboard 360 is a multi-agent LMS, which has been designed to facilitate learning through the use of leading-edge technology. Such system provides users with a **synchronous access to educational resources** from any device with Internet connection.\n",
    "\n",
    "The data is collected using a learner **activity tracker tool**, which called experience API (xAPI). The xAPI is a component of the training and learning architecture (TLA) that enables to monitor learning progress and learnerâs actions like reading an article or watching a training video. \n",
    "\n",
    "The dataset consists of **480 student records and 16 features**. The features are classified into three major categories: \n",
    "1. Demographic features such as gender and nationality. \n",
    "2. Academic background features such as educational stage, grade level and section.\n",
    "3. Behavioral features such as raised hand on class, opening resources, answering survey by parents, and school satisfaction.\n",
    "\n",
    "The dataset consists of **305 males and 175 females**. The students come from different origins such as 179 students are from Kuwait, 172 students are from Jordan, 28 students from Palestine, 22 students are from Iraq, 17 students from Lebanon, 12 students from Tunis, 11 students from Saudi Arabia, 9 students from Egypt, 7 students from Syria, 6 students from USA, Iran and Libya, 4 students from Morocco and one student from Venezuela.\n",
    "\n",
    "The dataset is collected through **two educational semesters**: 245 student records are collected during the first semester and 235 student records are collected during the second semester.\n",
    "\n",
    "The data set includes also the **school attendance feature** such as the students are classified into two categories based on their absence days: 191 students exceed 7 absence days and 289 students their absence days under 7.\n",
    "\n",
    "This dataset includes also a new category of features; this feature is **parent parturition in the educational process**. Parent participation feature have two sub features: Parent Answering Survey and Parent School Satisfaction. There are 270 of the parents answered survey and 210 are not, 292 of the parents are satisfied from the school and 188 are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/studentgrades.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**? \n",
    "- Gender - student's gender (nominal: 'Male' or 'Femaleâ)\n",
    "- Nationality- student's nationality (nominal:â Kuwaitâ,â Lebanonâ,â Egyptâ,â SaudiArabiaâ,â USAâ,â Jordanâ,â\n",
    "Venezuelaâ,â Iranâ,â Tunisâ,â Moroccoâ,â Syriaâ,â Palestineâ,â Iraqâ,â Lybiaâ)\n",
    "- Place of birth- student's Place of birth (nominal:â Kuwaitâ,â Lebanonâ,â Egyptâ,â SaudiArabiaâ,â USAâ,â Jordanâ,â\n",
    "Venezuelaâ,â Iranâ,â Tunisâ,â Moroccoâ,â Syriaâ,â Palestineâ,â Iraqâ,â Lybiaâ)\n",
    "- Educational Stages- educational level student belongs (nominal: âlowerlevelâ,âMiddleSchoolâ,âHighSchoolâ)\n",
    "- Grade Levels- grade student belongs (nominal: âG-01â, âG-02â, âG-03â, âG-04â, âG-05â, âG-06â, âG-07â, âG-08â, âG-09â, âG-10â, âG-11â, âG-12 â)\n",
    "- Section ID- classroom student belongs (nominal:âAâ,âBâ,âCâ)\n",
    "- Topic- course topic (nominal:â Englishâ,â Spanishâ, âFrenchâ,â Arabicâ,â ITâ,â Mathâ,â Chemistryâ, âBiologyâ, âScienceâ,â Historyâ,â Quranâ,â Geologyâ)\n",
    "- Semester- school year semester (nominal:â Firstâ,â Secondâ)\n",
    "- Parent responsible for student (nominal:âmomâ,âfatherâ)\n",
    "- Raised hand- how many times the student raises his/her hand on classroom (numeric:0-100)\n",
    "- Visited resources- how many times the student visits a course content(numeric:0-100)\n",
    "- Viewing announcements-how many times the student checks the new announcements(numeric:0-100)\n",
    "- Discussion groups- how many times the student participate on discussion groups (numeric:0-100)\n",
    "- Parent Answering Survey- parent answered the surveys which are provided from school or not\n",
    "(nominal:âYesâ,âNoâ)\n",
    "- Parent School Satisfaction- the Degree of parent satisfaction from school(nominal:âYesâ,âNoâ)\n",
    "- Student Absence Days-the number of absence days for each student (nominal: above-7, under-7)\n",
    "\n",
    "What is the **target**? Class\n",
    "- L (Low-Level): interval includes values from 0 to 69,\n",
    "- M (Middle-Level): interval includes values from 70 to 89,\n",
    "- H (High-Level): interval includes values from 90-100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.1: Plot the \"Class\" distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a plot to show how many students each level of \"Class\" includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "sns.countplot(x='Class', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.2: Preparing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encode categorical variables in **data**. \n",
    "\n",
    "**Hint**: ScikitLearn has nice classes for this purpose, namely [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) or [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), but you could do it also manually. \n",
    "\n",
    "You could go deeply into these two encoding strategies with [this supporting tutorial](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd).\n",
    "\n",
    "Based on the model you use, the decision on the encoding strategy becomes crucial. \n",
    "\n",
    "If you are experiencing difficulties in this step, you could skip it and put the only numerical features of **data** (raisedhands, VisITedResources, AnnouncementsView, Discussion) in **X** in the next step. For convenience, we cover the latter case here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "data = data[['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion', 'Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate features in **X** and the target variable in **y**.\n",
    "\n",
    "- Pick the column \"Class\" and copy its values into **y**. \n",
    "- Pick all the (other) features and copy them into **X** (if you have not encoded categorical values in **data** in the previous step, you should put only numerical features of **data**, namely raisehands, VisITedResources, AnnouncementsView, Discussion, into **X**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test sets, namely **X_train**, **X_test**, **y_train**, **y_test**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data in **X_train** and **X_test**.\n",
    "\n",
    "- Is scaling important in the context of this dataset? How this decision might influence the other decisions along the pipeline?\n",
    "- Which scaling approach would you select? Why?\n",
    "- Is your decision at this point influenced by the model you will choose to train? You might need to come back later at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL (this cell is just included for demonstration; we use RandomForest, so no scaling needed, skip it) ### \n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.3: Train a model\n",
    "\n",
    "Based on the characteristics of the dataset, choose a model among those we have presented and train it.\n",
    "You could start by training it with the default hyper-parameter values or with a combination of hyper-parameters you think it might make sense to start with. \n",
    "\n",
    "In the last task of this tutorial, you will play with other combinations of the different hyper-parameter values. \n",
    "\n",
    "- Which model have you selected? Why?\n",
    "- Have you used just the default values for the hyper-parameters? If so, why?\n",
    "- If you have you used non-default hyper-parameters, why have you picked them?\n",
    "\n",
    "Based on the model you selected, please consult the corresponding page in Scikit-learn to read and understand the hyper-parameters you could play with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.4: Make predictions\n",
    "\n",
    "- Compute predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.5: Calculate accuracy\n",
    "\n",
    "- Compute the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESERCISE CELL ###\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to go further #1\n",
    "\n",
    "Explore how the performance of your classification model varies, based on the value assigned to the important hyper-parameters.\n",
    "\n",
    "- Based on the model you selected, pick **one** hyper-parameter you think that is important\n",
    "- Select a range of values where the selected hyper-parameter will vary\n",
    "- Train a range of models with the different hyper-parameter values you selected\n",
    "- Create a plot with the range of hyper-parameters you tested on the x-axis and the accuracy score of the associated model on the y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How you go further #2\n",
    " \n",
    "During (if you have still time) or after the tutorial, you could go further by:\n",
    "- Running your pipeline for other classification models and compare accuracy across algorithms. \n",
    "- Investigating the most important features for this classification task. How could you do this?\n",
    "- Understanding deeply the data through an exploratory analysis and use your findings to improve the accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we have presented the basics of data generation, train-test split, scaling with the scikit-learn library. Then, we have introduced a range of classification models and how they can be implemented, trained, and tested in the same library. Finally, we have investigated how to operationalize a classification pipeline with an example dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
