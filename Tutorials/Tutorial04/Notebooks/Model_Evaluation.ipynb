{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4.1 - Model Evaluation\n",
    "\n",
    "The performance metrics that you choose to evaluate your machine learning algorithms are very important.\n",
    "\n",
    "Choice of performance metrics influences how the performance of machine learning algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose.\n",
    "\n",
    "**Expected Tasks**\n",
    "\n",
    "- Follow the performance metrics showcase.\n",
    "- Play with the different performance metric variants. \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Choosing the right performance metric according to your task and data. \n",
    "- Computing a performance metric in scikit-learn, given a certain evaluation method. \n",
    "\n",
    "**Notes**\n",
    "\n",
    "Performance metrics are demonstrated in this notebook using small code recipes in Python and  scikit-learn.\n",
    "\n",
    "More information on performance metrics supported by ScikitLearn are listed on the page [Model evaluation: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Supporting packages\n",
    "from sklearn import datasets, metrics, model_selection\n",
    "\n",
    "# Model packages\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We already discussed why generating datasets for different purposes, such as regression and classification. Now, we can see how to this for classification, and you this sample dataset to showcase how you can compute the different performance metrics covered in the lecture. \n",
    "\n",
    "Please, note that information on synthetic data generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=100, n_features=10, n_redundant=0, n_informative=10, class_sep=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding, we just show some examples for the features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and for the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Classication\n",
    "\n",
    "Classification problems are perhaps the most common type of machine learning problem and, as such, there is wide range of performance metrics that can be used to evaluate predictions for these problems. On the other hand, having all these performance  metrics will require to carefully understand, motivate, and discuss the performance metrics you think that are more relevant for your problem, according to the data and the model you selected.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will review how to compute and report:\n",
    "\n",
    "- Accuracy.\n",
    "- Balanced Accuracy. \n",
    "- Precision.\n",
    "- Recall. \n",
    "- F-Measure.\n",
    "- Area Under the ROC Curve (AUC).\n",
    "- Confusion Matrix.\n",
    "- Classification Report.\n",
    "\n",
    "\n",
    "*Note that your results may vary given the stochastic nature of the evaluation procedure or the differences in numerical precision, based on your device.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the lecture, you may need to compute performance metrics on a training-test split or through a re-sampling method. In the tutorial, due to time constraints, we will see how to compute performance metrics for an example training-test split and for an example cross-validation strategy, both on a RandomForest classifier. In any case, Please note that you can easily adapt our showcase to the other cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_cv = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2,  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The accuracy represents the percentage of correct predictions your model has made. This is one of the most common and intuitive evaluation metric for classification problems. However, it is also the most misused. It is really only suitable when there is an equal number of observations in each class (which is rarely the case) and when all predictions and prediction errors are equally important (which is rarely the case). Below is an example of calculating accuracy.\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='accuracy')\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy: %.3f\" % (metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy \n",
    "\n",
    "The accuracy is the total number of correct predictions divided by the total number of predictions made for a dataset. As a performance measure, so accuracy is inappropriate for imbalanced classification problems. The main reason is that the number of examples from the majority class (or classes) may be far higher than the number of examples in the minority class. Hence, if we have a dataset with 90% of the samples from the majority class, a model that always predicts the majority class can achieve an accuracy score of 90%.\n",
    "\n",
    "The balanced accuracy is an alternative metric to accuracy in binary and multiclass classification problems, suitable in case of imbalanced datasets. It is defined as the average number of predictions correctly classified *per class* (i.e., it is the average recall per class). \n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='balanced_accuracy')\n",
    "print(\"Balanced Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Balanced Accuracy: %.3f\" % (metrics.balanced_accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The confusion matrix is a handy presentation of the accuracy of a model for two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table include the percentage of predictions made correctly by a machine learning algorithm for that class. For example, a binary classifier can predict 0 or 1, and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for True label=0 and Predicted label=0, whereas predictions for 0 that were actually 1 appear in the cell for True label= 0 and Predicted label=1, and so on.\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "metrics.plot_confusion_matrix(model, X_test, y_test, display_labels=np.arange(0, 2), cmap=plt.cm.Blues, normalize='true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "An alternative to accuracy in case of imbalanaced problems is represented by precision and recall metrics. Precision and recall might be useful in cases where there is an imbalance in the observations between the two (or more) classes. For instance, let's consider a dataset where there are many examples of no event (class 0) and only a few examples of an event (class 1). In this cases, having the large number of class 0 examples means that we are less interested in the ability of the model at predicting class 0 correctly, e.g. high true negatives. The key to the calculation of precision and recall is so that the calculations do not make use of the true negatives, and it is only based on the correct prediction of the minority class, class 1.\n",
    "\n",
    "$P = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$R = \\frac{TP}{TP + FN}$ \n",
    "\n",
    "Mode details on [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) and [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) are provided in the scikit-learn documentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='precision')\n",
    "print(\"Precision: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Precision: %.3f\" % (metrics.precision_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='recall')\n",
    "print(\"Recall: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Recall: %.3f\" % (metrics.recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may decide to use precision or recall on your imbalanced classification problem. Maximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives. Therefore:\n",
    "\n",
    "- Precision might be more appropriate when minimizing false positives is the focus.\n",
    "- Recall might be more appropriate when minimizing false negatives is the focus.\n",
    "\n",
    "Sometimes, you may want good predictions of the positive class. We want high precision and high recall. This can be challenging, as often increases in recall often come at the expense of decreases in precision. In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since to increase the true positives for the minority class, the number of false positive is also increased, resulting in reduced precision.\n",
    "\n",
    "Your question might be what you could do in case you have a multi-class problem. The average paremeter of the metric functions (e.g., precision and recall) is required for multiclass/multilabel targets to determine the type of averaging performed on the data:\n",
    "- 'binary' only reports results for the class specified by pos_label, and is applicable only if targets are binary.\n",
    "- 'micro' calculates metrics globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "- 'weighted': calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure or F1\n",
    "\n",
    "F-Measure or F1 score provides a way to combine both precision and recall into a single measure that captures both properties. Unfortunately, neither precision or recall tells the whole story. You may have a very high precision and a very low recall, or alternately, a very low precision with a very high recall. F1 provides a way to express both performance  metrics with a single score. Once precision and recall have been calculated for a binary or multiclass classification problem, the two scores can be combined into the calculation of the F-Measure.\n",
    "\n",
    "$F1 = \\frac{2 * Precision * Recall}{Precision + Recall}$. \n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='f1')\n",
    "print(\"F1: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"F1: %.3f\" % (metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under ROC Curve \n",
    "\n",
    "The AUC represents the model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of probability predictions at different thresholds used to map the probabilities to class labels. The AUC score thus indicates the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestClassifier(random_state=0), X, y, cv=kfold_cv, scoring='roc_auc')\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"AUC: %.3f\" % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve is the approximate integral under the ROC Curve. For more information on ROC Curves and ROC AUC, see [this tutorial](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). Here, we show you a short example on how to plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([-1,1],[-1,1], 'red', linestyle='--', linewidth=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the majority of the churns are wrongly classified (true label 1, predicted label 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "Scikit-learn provides a convenience report when working on classification to give you a quick idea of the accuracy of a model using a number of measures.\n",
    "\n",
    "The classification_report() function displays the precision, recall, f1-score and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics.classification_report(y_test, y_pred, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifiers\n",
    "\n",
    "Scikitlearn provides handy classifiers that make predictions using simple rules. This kind of classifiers is useful as a simple baseline to compare with other (real) classifiers. For instance, you could define a classifier that generates predictions by respecting the training set’s class distribution or a classifier that always predicts the most frequent label in the training set.\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(random_state=0, strategy='most_frequent')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy: %.3f\" % (metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Regression\n",
    "\n",
    "In this section will review four of the most common performance metrics for evaluating predictions on regression machine learning problems:\n",
    "\n",
    "- Mean Absolute Error.\n",
    "- (Root) Mean Squared Error.\n",
    "- R^2.\n",
    "\n",
    "We will use here a synthetic dataset for regression. More information on synthetic data generation can be found in the [ScikitLearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=100, n_features=10, noise=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for a better understanding, we just show some examples for the features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we scale target values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y - y.min()) / (y.max() - y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in this case, we will see how to compute performance metrics for an example training-test split and for an example cross-validation strategy, both on a RandomForest regressor. In any case, Please note that you can easily adapt our showcase to the other cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_cv = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2,  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "The Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestRegressor(random_state=0), X, y, cv=kfold_cv, scoring='neg_mean_absolute_error')\n",
    "print(\"MAE: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"MAE: %.3f\" % (metrics.mean_absolute_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0 indicates no error or perfect predictions. Please, note that this metric is inverted by the cross_val_score() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "The Root Mean Squared Error (RMSE) is a quadratic scoring rule which measures the average magnitude of the error. Expressing the formula in words, the difference between the prediction and the true value are each squared and then averaged over the sample. Finally, the square root of the average is taken. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable. \n",
    "\n",
    "More details onthe [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestRegressor(random_state=0), X, y, cv=kfold_cv, scoring='neg_root_mean_squared_error')\n",
    "print(\"RMSE: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE: %.3f\" % (metrics.mean_squared_error(y_test, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric too is inverted so that the results are increasing. To obtain the Mean Squared Error (MSE), the squared parameter should be True. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R^2 Metric\n",
    "The R^2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively.\n",
    "\n",
    "More details on the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cross-validation scenario \n",
    "results = model_selection.cross_val_score(RandomForestRegressor(random_state=0), X, y, cv=kfold_cv, scoring='r2')\n",
    "print(\"R2: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "# Example for a training-test split\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R2: %.3f\" % (metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you discovered performance metrics that you can use to evaluate your machine-learning models, covering both classification and regression cases. Furthermore, you have seen how to compute them in scikit-learn. Please, rememeber that the choice of the performance metrics depends on the data and the problem at your hands. Therefore, you should be able to justify what you picked a given set of performance metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
