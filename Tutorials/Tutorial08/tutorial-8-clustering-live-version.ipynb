{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fourth-contemporary",
   "metadata": {},
   "source": [
    "# Tutorial 8 - Structure Discovery (Live version)\n",
    "\n",
    "**Only use this version if you are following the tutorial on Wednesday**\n",
    "\n",
    "In class, we covered three different clustering algorithms (K-Means Clustering, Spectral Clustering, and Hierarchical Agglomerative Clustering) and different heuristics for choosing the optimal number of clusters (Elbow method, Silhouette, BIC, eigengap, dendrogram). \n",
    "\n",
    "In this tutorial, you will get hands-on experience on the three algorithms and explore the factors that influence the heuristics to choose the optimal number of clusters. \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Understanding each Clustering Algorithm and its assumptions.\n",
    "- Exploring the different heuristics and their robustness.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Unless otherwise stated, you may use any existing libraries and methods to solve the tasks. \n",
    "\n",
    "\n",
    "**Live tutorial**\n",
    "\n",
    "* We will use requests to share examples. When possible, use matplotlib and do not return the plot with plt.show()\n",
    "* The live solutions will be posted [here](https://drive.google.com/file/d/1qiWWA0_AoYoSJoBB68m-y4pwpuqpIRq5/view?usp=sharing). Refresh constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "exec(requests.get(\"https://courdier.pythonanywhere.com/get-send-code\").content)\n",
    "\n",
    "npt_config = {\n",
    "    'session_name': 'lab-08',\n",
    "    'session_owner': 'mlbd',\n",
    "    'sender_name': input(\"Write your name or alias:\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-plaintiff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e51241a91337b40e4d605cd378c190ea",
     "grade": false,
     "grade_id": "cell-a58362ff54246f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add imports here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "SEED = 111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-flood",
   "metadata": {},
   "source": [
    "## 0. Simulated Data\n",
    "---\n",
    "One of the main objectives of this tutorial is to explore clustering algorithms applied to different datasets. In the next cell, we are generating 4 different datasets (blobs, circles, moons, and ovals). These are just a few examples of datasets that can be explored in the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "clusters = 4\n",
    "X_blobs, y_blobs = make_blobs(n_samples=n_samples,centers = clusters,\n",
    "                  cluster_std = 0.3, random_state=SEED)\n",
    "X_circles, y_circles = make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "X_moons, y_moons = make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_trans = np.dot(X_blobs, transformation)\n",
    "y_trans = y_blobs\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5), dpi=80)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], s=50, c = y_blobs)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], s=50, c = y_circles)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], s=50, c = y_moons)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(X_trans[:, 0], X_trans[:, 1], s=50, c = y_trans)\n",
    "\n",
    "send(plt, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-public",
   "metadata": {},
   "source": [
    "## 1. K-Means Clustering\n",
    "---\n",
    "In this tutorial, we will use simulated data to understand the behavior of different clustering algorithms and the heuristics to choose the optimal number of clusters. Letâ€™s start by exploring K-Means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "X, y = make_blobs(n_samples=n_samples,centers = 4,\n",
    "                  cluster_std = 1, random_state=SEED)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-pleasure",
   "metadata": {},
   "source": [
    "### 1.1 Cluster the data using K-Means and plot the resulting clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-emphasis",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bfc746f8f3f6ed0e407e5b5ec7f0846",
     "grade": false,
     "grade_id": "cell-d49d0ee28e6a1f0d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "send(plt, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-satin",
   "metadata": {},
   "source": [
    "### 1.2 Optimal number of clusters\n",
    "As seen in class there are several heuristics to determine the optimal number of clusters. In this tutorial, we will compare them and analyze how they behave with various datasets, variations within the clusters, and the number of samples. But first, we will calculate the heuristics individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-blame",
   "metadata": {},
   "source": [
    "#### 1.2.1 Elbow method\n",
    "\n",
    "In the elbow method, we want to choose k* such that adding another cluster does not lead to a much better model of the data. We do this by measuring the distortion  (in-cluster variance) defined as: \n",
    "$$\n",
    "D=\\sum_{n}\\left(d\\left(\\boldsymbol{m}{\\widehat{\\boldsymbol{k}}^{n}, \\boldsymbol{x}{\\boldsymbol{n}}}\\right)\\right)^{2}\n",
    "$$\n",
    "Your task is to complete the function plot_distortion. Plot the distortion on the y-axis and the number of clusters in the x-axis \n",
    "\n",
    "**Challenge**: Compute the distortion using the [lecture notes](https://moodle.epfl.ch/mod/resource/view.php?id=1143936)\n",
    "(Slide 31 Week 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-beginning",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96b65f27eed4b03b534fe62f3ee414cb",
     "grade": false,
     "grade_id": "cell-029a936d4364adcd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_distortion(n_clusters_list, X):\n",
    "    \"\"\"\n",
    "    Plot the distortion (in-cluster variance) on the y-axis and \n",
    "    the number of clusters in the x-axis \n",
    "    \n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    :param X: np array of data points \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "n_clusters_list = range(2,10)    \n",
    "plot_distortion(n_clusters_list, X)\n",
    "send(plt, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-realtor",
   "metadata": {},
   "source": [
    "#### 1.2.2 Average Silhouette Width\n",
    "\n",
    "Another heuristic seen in class was the Silhouette width which measures how similar a data point is to its own cluster (cohesion) compared to other clusters. Your task is to complete the function plot_silhouette. Plot the silhouette score on the y-axis and the number of clusters in the x-axis. \n",
    "\n",
    "**Challenge**: Compute the silhouette score using the [lecture notes](https://moodle.epfl.ch/mod/resource/view.php?id=1143936)  (Slide 33 Week 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-justice",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e98470529e3f1d148df9b80521db4b10",
     "grade": false,
     "grade_id": "cell-5ab2a59b63fec663",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_silhouette(n_clusters_list, X):\n",
    "    \"\"\"\n",
    "    Plot the silhouette score on the y-axis and\n",
    "    the number of clusters in the x-axis\n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    :param X: np array of data points \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "plot_silhouette(n_clusters_list, X)\n",
    "send(plt, 122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-drink",
   "metadata": {},
   "source": [
    "#### 1.2.3 BIC\n",
    "\n",
    "The third heuristic seen in class for K-Means is the Bayesian Information Criterion. Your task is to complete the functions compute_bic using the [lecture notes](https://moodle.epfl.ch/mod/resource/view.php?id=1148083) \n",
    "(Slide 4 Week 9) and the plot the BIC with the function plot_bic.\n",
    "You might find [these](https://github.com/bobhancock/goxmeans/blob/master/doc/BIC_notes.pdf) notes useful to optimize your code: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-cache",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63b322a9bc43a77034813f5f72f7c2b4",
     "grade": false,
     "grade_id": "cell-458752307f0bcb9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_bic(kmeans, X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric\n",
    "\n",
    "    :param kmeans: clustering object from scikit learn\n",
    "    :param X: np array of data points\n",
    "    :return: BIC\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return BIC\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=100,centers = 4,\n",
    "                  cluster_std = 1, random_state=SEED)\n",
    "kmeans = KMeans(n_clusters = 4, random_state = SEED).fit(X)\n",
    "bic = compute_bic(kmeans, X)\n",
    "send(str(bic), 1231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-intermediate",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fd34edb130efa1159ceb10b20e03351",
     "grade": false,
     "grade_id": "cell-85d08eb88203a337",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_bic(n_clusters_list, X):\n",
    "    \"\"\"\n",
    "    Plot the BIC on the y-axis and the number of clusters in the x-axis\n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    :param X: np array of data points \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "plot_bic(n_clusters_list, X)\n",
    "send(plt, 1232)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-length",
   "metadata": {},
   "source": [
    "#### 1.2.4 Plot all heuristics\n",
    "Plot all heuristics. Now that you have calculated all three heuristics, your task is to complete the function get_heuristics_kmeans that plots the three heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-bubble",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd43f8e0bef9c51899c119ab3581447",
     "grade": false,
     "grade_id": "cell-c1c3aa0dc08df5f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(n_clusters_list, metric_dictionary):\n",
    "    \"\"\"\n",
    "    Plots metric dictionary (auxilary function)\n",
    "    [Optional]\n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    :param metric_dictionary: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-costs",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3233d31bde4f15b69a82904368d84ca",
     "grade": false,
     "grade_id": "cell-b7740d596953b3ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_heuristics_kmeans(X, n_clusters_list = range(2,10)):\n",
    "    \"\"\"\n",
    "    Calculates heuristics for optimal number of clusters with K-Means \n",
    "    \n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    :param X: np array of data points \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "get_heuristics_kmeans(X, n_clusters_list)\n",
    "send(plt, 124)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-imagination",
   "metadata": {},
   "source": [
    "### 1.3 Heuristic analysis\n",
    "Your task is to play with the generated data and understand the robustness and sensitivity of each heuristic and answer the following questions:\n",
    "- How does the standard deviation affect each heuristic? Which heuristics are more robust to variation? Play with the standard deviation of the clusters (cluster_std)\n",
    "- How does the optimal number of clusters vary with the number of data points? Play with the number of samples (n_samples)\n",
    "- How does the algorithm behave with different data sets? Why? What are the assumptions of K-means? (See lecture notes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "clusters = 4\n",
    "X, y = make_blobs(n_samples=n_samples,centers = clusters,\n",
    "                  cluster_std =1, random_state=SEED)\n",
    "\n",
    "fig = plt.subplots(squeeze=False)\n",
    "ax1 = plt.scatter(X[:, 0], X[:, 1], s=50, c = y)\n",
    "ax2 =  get_heuristics_kmeans(X)\n",
    "\n",
    "\n",
    "# One example where the all metrics agree on the optimal number of clusters\n",
    "# send(plt, 1301)\n",
    "\n",
    "# One example where the some metrics disagree on the optimal number of clusters\n",
    "# send(plt, 1302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share your findings (optional)\n",
    "# How does the standard deviation affect each heuristic? Which heuristics are more robust to variation? Play with the standard deviation of the clusters (cluster_std)\n",
    "answer = \"\"\n",
    "send(answer, 131)\n",
    "\n",
    "# How does the optimal number of clusters vary with the number of data points? Play with the number of samples (n_samples)\n",
    "answer = \"\"\n",
    "send(answer, 132)\n",
    "\n",
    "# How does the algorithm behave with different data sets? Why? What are the assumptions of K-means? (See lecture notes) \n",
    "answer = \"\"\n",
    "send(answer, 133)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-influence",
   "metadata": {},
   "source": [
    "## 2. Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-officer",
   "metadata": {},
   "source": [
    "### 2.1 Cluster the data using Spectral Clustering and plot the resulting clusters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-saskatchewan",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7c16664106902395c2efbf246452d45",
     "grade": false,
     "grade_id": "cell-75ee2c09b8306ef8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "clusters = 4\n",
    "X, y = make_blobs(n_samples=n_samples, centers=clusters,\n",
    "                  cluster_std=1, random_state=SEED)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "send(plt, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-symposium",
   "metadata": {},
   "source": [
    "### 2.2 Build Spectral Clustering \n",
    "Scikit learn has a well-optimized implementation of Spectral Clustering however we cannot access the intermediate steps. If we want to compute the distortion, BIC, and eigengap heuristic, we must implement it ourselves. Recall that Spectral Clustering is K-Means clustering in a transformed space. Your task is to follow the [lecture notes](https://moodle.epfl.ch/mod/resource/view.php?id=1143936) (Slide 46 Week 6) to complete the spectral_clustering function.\n",
    "\n",
    "Compare your results to be ones obtained in 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-louisville",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc3008ff38744982ee33dcf3bb66f53a",
     "grade": false,
     "grade_id": "cell-1435f0115a437369",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spectral_clustering(X, n_clusters):\n",
    "    \"\"\"\n",
    "    Spectral clustering\n",
    "    :param X: np array of data points\n",
    "    :param n_clusters: number of clusters\n",
    "    :return: tuple (kmeans, proj_X, eigenvals_sorted)\n",
    "        WHERE\n",
    "        kmeans scikit learn clustering object\n",
    "        proj_X is np array of transformed data points\n",
    "        eigenvals_sorted is np array with ordered eigenvalues \n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return kmeans, proj_X, eigenvals_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans, proj_X, eigenvals_sorted  = spectral_clustering(X,  clusters)\n",
    "y_pred = kmeans.labels_\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = y_pred)\n",
    "send(plt, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-clark",
   "metadata": {},
   "source": [
    "### 2.3 Optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-excuse",
   "metadata": {},
   "source": [
    "#### 2.3.1 Eigengap Heuristic\n",
    "As seen during the lecture, for Spectral Clustering the Eigengap Heuristic can be used to choose k* so that the first k eigenvalues are very small ut k+1 is relatively large. Your task is to complete the function plot_eigengap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-mills",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a268d8af7a12850e42229786c1264a6",
     "grade": false,
     "grade_id": "cell-7e7770881ac865ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_eigengap(eigenvals_sorted):\n",
    "    \"\"\"\n",
    "    :param eigenvals_sorted: np array with ordered eigenvalues \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "plot_eigengap(eigenvals_sorted)\n",
    "send(plt, 231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-saturday",
   "metadata": {},
   "source": [
    "#### 2.3.2 Plot all heuristics\n",
    "Similarly to what you did with K-Means, complete the function get_heuristics_spectral that plots the four heuristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-buyer",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cffdeb5e41a65def86d3a421c5fd34d4",
     "grade": false,
     "grade_id": "cell-57db867a47c28363",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_metrics(n_clusters_list, metric_dictionary):\n",
    "    \"\"\"\n",
    "    Plots metric dictionary (auxilary function)\n",
    "    [Optional]\n",
    "    \n",
    "    :param n_clusters_list: List of number of clusters to explore \n",
    "    :param metric_dictionary: \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-impact",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb9a0a4de5d4025858632c8de5e2eb4c",
     "grade": false,
     "grade_id": "cell-0459eea736bb2c8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_heuristics_spectral(X, n_clusters_list=range(2, 8)):\n",
    "    \"\"\"\n",
    "    Calculates heuristics for optimal number of clusters with Spectral Clustering\n",
    "    \n",
    "    :param X: np array of data points\n",
    "    :param n_clusters_list: List of number of clusters to explore\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "get_heuristics_spectral(X, n_clusters_list)\n",
    "send(plt, 232)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-television",
   "metadata": {},
   "source": [
    "### 2.4 Heuristic analysis\n",
    "Your task is to play with the generated data and understand the robustness and sensitivity of each heuristic\n",
    "- How does the standard deviation affect each heuristic? Which heuristics are more robust to variation? Play with the standard deviation of the clusters (cluster_std)\n",
    "- How does the optimal number of clusters vary with the number of data points? Play with the number of samples (n_samples)\n",
    "- How does the algorithm behave with different data sets? Why? What are the assumptions of Spectral Clustering? (See lecture notes) \n",
    "- How is Spectral Clustering different from K-Means?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "clusters = 4\n",
    "X, y = make_blobs(n_samples=n_samples,centers = clusters,\n",
    "                  cluster_std =0.6, random_state=SEED)\n",
    "\n",
    "fig = plt.subplots(squeeze=False)\n",
    "ax1 = plt.scatter(X[:, 0], X[:, 1], s=50, c = y)\n",
    "ax2 =  get_heuristics_spectral(X, n_clusters_list)\n",
    "\n",
    "\n",
    "# One example where the all metrics agree on the optimal number of clusters\n",
    "# send(plt, 2401)\n",
    "\n",
    "# One example where the some metrics disagree on the optimal number of clusters\n",
    "# send(plt, 2402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share your findings (optional)\n",
    "# How does the standard deviation affect each heuristic? Which heuristics are more robust to variation? Play with the standard deviation of the clusters (cluster_std)\n",
    "answer = \"\"\n",
    "send(answer, 241)\n",
    "\n",
    "# How does the optimal number of clusters vary with the number of data points? Play with the number of samples (n_samples)\n",
    "answer = \"\"\n",
    "send(answer, 242)\n",
    "\n",
    "# How does the algorithm behave with different data sets? Why? What are the assumptions of Spectral Clustering? (See lecture notes) \n",
    "answer = \"\"\n",
    "send(answer, 243)\n",
    "\n",
    "# How is Spectral Clustering different from K-Means?\n",
    "answer = \"\"\n",
    "send(answer, 244)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-recommendation",
   "metadata": {},
   "source": [
    "## 3 Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-hormone",
   "metadata": {},
   "source": [
    "### 3.1 Cluster the data using Hierarchical Agglomerative Clustering and plot the resulting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-stand",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d30d0c7e8ed6b69f4a0441a519b004f",
     "grade": true,
     "grade_id": "cell-3dea870635428768",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "clusters = 7\n",
    "X, y = make_blobs(n_samples=n_samples,centers = clusters,\n",
    "                  cluster_std = 0.7, random_state=SEED)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "send(plt, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-identification",
   "metadata": {},
   "source": [
    "### 3.2 Linkages\n",
    "As seen in class there are different ways to identify the closest clusters (single, complete, average, centroid, ward). Your task is to explore how the different linkages behave with changes in the dataset (variance, shape, number of data points). What conclusions can you draw?\n",
    "\n",
    "**Challenge** Compute the execution times. Which linkage is the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-trailer",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2efffb63daa4933261d3b37b0b498a3",
     "grade": true,
     "grade_id": "cell-90e67ebb4b6db35a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_linkages(clusters, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "clusters = 4\n",
    "X, y = make_blobs(n_samples=n_samples,centers = clusters,\n",
    "                  cluster_std =1.6, random_state=SEED)\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = y)\n",
    "plt.title('Original Data')\n",
    "plt.show()\n",
    "\n",
    "compare_linkages(clusters, X)\n",
    "send(plt, 321)\n",
    "\n",
    "# One example where the simple linkage outperforms the other linkages\n",
    "# send(plt, 322)\n",
    "\n",
    "# One example where the simple linkage underperforms comparared to the other linkages\n",
    "# send(plt, 323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What conclusions can you draw?\n",
    "answer = \"\"\n",
    "send(answer, 324)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-clear",
   "metadata": {},
   "source": [
    "### 3.3 Dendrogram \n",
    "Visualize the Dendrogram.\n",
    "\n",
    "**Hint:** Set the number of clusters to None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-california",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67694f2b83c30db39cb3fc9f2af9c2a6",
     "grade": false,
     "grade_id": "cell-9f728f8eec2521e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model):\n",
    "    \"\"\"\n",
    "    Create linkage matrix and then plot the dendrogram\n",
    "    \n",
    "    :param model: clustering object from scikit learn\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "clusters = 5\n",
    "n_samples = 300\n",
    "X, y = make_blobs(n_samples=n_samples, centers=clusters,\n",
    "                  cluster_std=1, random_state=SEED)\n",
    "\n",
    "hierarchical = AgglomerativeClustering(distance_threshold=0,\n",
    "                                       linkage='ward',\n",
    "                                       n_clusters=None).fit(X)\n",
    "plot_dendrogram(hierarchical)\n",
    "send(plt,33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-composer",
   "metadata": {},
   "source": [
    "### 3.4 Optimal number of clusters\n",
    "\n",
    "Using the dendrogram we can find the optimal number of clusters. You must:\n",
    "1. Locate the largest vertical line (without crossing any horizontal line) \n",
    "2. Draw a horizontal line just before the end of the largest vertical line\n",
    "3. Count the number of vertical lines that the horizontal line from (2) intercepts\n",
    "Your task is to explore how truth this holds with variations in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "n_samples = 300\n",
    "X, y = make_blobs(n_samples=n_samples, centers=clusters,\n",
    "                  cluster_std=1, random_state=SEED)\n",
    "#X = X_moons\n",
    "hierarchical = AgglomerativeClustering(distance_threshold=0,\n",
    "                                       linkage='ward',\n",
    "                                       n_clusters=None).fit(X)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), dpi=80)\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_dendrogram(hierarchical)\n",
    "\n",
    "\n",
    "hierarchical = AgglomerativeClustering(linkage='ward',\n",
    "                                       n_clusters=clusters).fit(X)\n",
    "y_pred = hierarchical.fit_predict(X)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c = y_pred)\n",
    "\n",
    "\n",
    "# One example where the dendrogram method works\n",
    "# send(plt, 341)\n",
    "\n",
    "# One example where the dendrogram method does NOT work\n",
    "# send(plt, 342)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-genetics",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you used three different clustering algorithms (K-Means Clustering, Spectral Clustering, and Hierarchical Agglomerative Clustering) and hopefully learned the intuition behind different heuristics for choosing the optimal number of clusters (Elbow method, Silhouette, BIC, eigengap, dendrogram). \n",
    "\n",
    "**Lab discussion** \n",
    "In which situations would you use each algorithm? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In which situations would you use each algorithm? \n",
    "answer = \"K-Means when ...\"\n",
    "send(answer, 41)\n",
    "\n",
    "answer = \"Spectral Clustering when ...\"\n",
    "send(answer, 42)\n",
    "\n",
    "answer = \"Hierarchical Agglomerative Clustering when ...\"\n",
    "send(answer, 43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-sender",
   "metadata": {},
   "source": [
    "Please give us feedback [here](https://moodle.epfl.ch/mod/questionnaire/view.php?id=1147889)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
