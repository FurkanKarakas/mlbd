{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlQ--Xoco-FA"
   },
   "source": [
    "# Tutorial 8 - Recommender Systems Part II  \n",
    "\n",
    "Last week, we looked at loading and exploring datasets with users' feedback and played with a range of recommendation techniques, such as non-personalized approaches and traditional collaborative filtering approaches. We also introduced simple evaluation settings for rating prediction tasks on top of the Surprise package. This tutorial explores an alternative technique for collaborative filtering using latent factor models, with simple TensorFlow recipes. Furthermore, this tutorial covers a range of how to perform a train-test split, the creation of recommended lists, and the computation of decision-support and rank-aware top-k metrics that are fundamental for the evaluation of recommender systems.   \n",
    "\n",
    "**Expected Tasks**\n",
    "\n",
    "- Follow the latent factor models showcase in TensorFlow.\n",
    "- Solve a range of exercises throughout the notebook. \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Load and explore dataset with users' feedback. \n",
    "- Implement a train-test split technique. \n",
    "- Create and optimize a latent factor model.\n",
    "- Compute the recommended list of a given user. \n",
    "- Evaluate a recommender systems, using different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NAa-SicMd7B"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import time \n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7hbi1zgo7XR"
   },
   "source": [
    "## Step #1: Load the Movielens 1M data set\n",
    "---\n",
    "\n",
    "**Reference**: Harper, F. M., & Konstan, J. A. (2015). The Movielens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TIIS), 5(4), 1-19. [https://dl.acm.org/doi/10.1145/2827872](https://dl.acm.org/doi/10.1145/2827872). \n",
    "\n",
    "The MovieLens datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of <user, item, rating, timestamp> tuples, each the result of a person expressing a preference (a 0–5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens website, a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations. \n",
    "\n",
    "The MovieLens datasets are heavily downloaded and referenced in the research literature. This popularity is, to a certain degree, a reflection of the incredible rate of growth of personalization and recommendation research, in which datasets such as these have substantial value in exploring and validating ideas. The popularity might also be attributed to the flexibility of ratings data. Also, because movie preferences are highly subject to personal tastes, the movie domain is well suited to testing personalization technology. Finally, the popularity may reflect the perceived accessibility of movies as a content domain: movies are a common interest, making algorithmic output easy to discuss. \n",
    "\n",
    "In this tutorial, we will play with a small version of a dataset collected from the Movielens platform. Specifically, we will use the MovieLens 1M movie ratings dataset ([https://grouplens.org/datasets/movielens/1m/](https://grouplens.org/datasets/movielens/1m/)), with 1 million ratings from 6000 users on 4000 movies. The dataset includes information about **ratings**, **movies**, **users**. \n",
    "\n",
    "All **ratings** are contained in the file \"ratings.dat\" and are in the following format: user_id::item_id::rating::timestamp. \n",
    "- User IDs range between 1 and 6040. \n",
    "- Item IDs range between 1 and 3952.\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only). \n",
    "- Timestamp is represented in seconds since the epoch as returned by time. \n",
    "- Each user has at least 20 ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4onM3ObBla4m"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'],engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "z7N5crOymEcU",
    "outputId": "b57c584a-e693-436b-b382-236ccbe17b9f"
   },
   "outputs": [],
   "source": [
    "ratings.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEIdZKJOmGmD",
    "outputId": "62766cf9-2d66-4086-9ca6-ccb526c0646f"
   },
   "outputs": [],
   "source": [
    "no_users, no_items, no_ratings = len(ratings.user_id.unique()), len(ratings.item_id.unique()), len(ratings.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_users, no_items, no_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the movies. All movies information is in the file \"movies.dat\" and is in the following format: item_id::title::genres. \n",
    "\n",
    "- Titles are identical to titles provided by the IMDB (including year of release). \n",
    "- Genres are pipe-separated and are selected from the following genres (Action, Adventure, Animation, Children's, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War, Wester). \n",
    "- Some MovieIDs do not correspond to a movie due to accidental duplicate entries and/or test entries. \n",
    "- Movies are mostly entered by hand, so errors and inconsistencies may exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('ml-1m/movies.dat', sep='::', names=['item_id', 'title', 'genres'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIvKhIjksFZR"
   },
   "source": [
    "For convenience, we re-scale the user ids from $0$ to $|\\text{no_users}-1|$ and the item ids from $0$ to $|\\text{no_items}-1|$. This pre-processing step will allow us to use the id as an index in the latent factors matrices of users and items. For convenience, we also keep two columns with the original user ids and item ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejwUFstRsINf"
   },
   "outputs": [],
   "source": [
    "ratings['original_user_id'] = ratings['user_id']\n",
    "ratings['original_item_id'] = ratings['item_id']\n",
    "ratings['user_id'] = ratings['user_id'].astype('category').cat.codes\n",
    "ratings['item_id'] = ratings['item_id'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ratings['user_id']), max(ratings['user_id']), min(ratings['item_id']), max(ratings['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO5eP_o1pEqn"
   },
   "source": [
    "## Step #2: Define the evaluation method\n",
    "---\n",
    "\n",
    "Before delving into latent factor models, as we have seen in the lectures, we need to decide the **evaluation method** to apply in our analysis. The evaluation method in use depends on the data and the context. For instance, in some cases, you may opt for a **train-test split** (split data into a train and a test set). In other cases, a **cross-validation** might be more appropriate (split data into folds, hold out each in turn for testing). Once you decide the overall evaluation method, you need to define **how the data will be split** in train-test or across folds. Very common solutions require to sample a percentage of ratings to be included in the test set or to sample a fixed percentage or number (same for all) of ratings for each user in the test set. Another strategy is based on sampling a percentage of users (first-level) whose ratings are included in the training set and, for the users in the test set, applying another splitting strategies (second-level). Furthermore, such a split may be done randomly or by considering the timestamps of when user-item interactions were performed. \n",
    "\n",
    "### Exercise #1 \n",
    "\n",
    "In the following cell, we ask you to split the interactions in <code>ratings</code> in two disjoint dataframes <code>train_ratings</code> and <code>test ratings</code>, according to a train-test split strategy that takes the 20% most recent ratings for each user in the test set, while the rest of the ratings are added to the training set. During or after the tutorial, in the following cell, you could also implement other strategies among the ones listed above and inspect how the performance varies according to the splitting strategy; you just need to re-run the other cells then.   \n",
    "\n",
    "Note that <code>train_ratings</code> and <code>test ratings</code> should have the same four columns of the original dataframe <code>ratings</code>. The train and test dataframes just differ in terms of rows (i.e., interactions).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g65rlT51mN0L"
   },
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wre9zSDmdUT",
    "outputId": "f7cebb03-cbcc-44e2-eff6-3104247e694e"
   },
   "outputs": [],
   "source": [
    "len(train_ratings.index), len(train_ratings['user_id'].unique()), len(train_ratings['item_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTvMDqxemqKp",
    "outputId": "fb18958d-efae-4cc5-843d-fa7feed2f244"
   },
   "outputs": [],
   "source": [
    "len(test_ratings.index), len(test_ratings['user_id'].unique()), len(test_ratings['item_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What pros and cons does this evaluation method (i.e., train-test split) have?\n",
    "- Is the selected splitting strategy a good simulation of a real-world situation? If not, why?\n",
    "- Do you have any argument on why the number of items is different between the training and test sets? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0XwyW27qB4y"
   },
   "source": [
    "## Step #3: Define the latent factor model\n",
    "---\n",
    "\n",
    "In the previous tutorial, we showed recommendation models aimed at factorizing a rating matrix based on dimensionality reduction techniques, such as Singular Value Decomposition (SVD). However, the rating matrix is only partially observed and it is really large. Methods like SVD are not defined for partially observed matrices, and they are not often practical for large matrices with thousands or millions of users and items. Latent factor models have been introduced to solve the matrix factorization problem approximately using (stochastic) gradient descent. Specifically, the general equation can be expressed as:\n",
    "\n",
    "$R = Q P^T$\n",
    "\n",
    "where: \n",
    "- $R$ is a rating matrix with $m$ items and $n$ users\n",
    "- $Q$ is an item latent factor matrix of size ($m, f$), with $f$ being the number of item factors.  \n",
    "- $P$ is an user latent factor matrix of size ($f, n$), with $f$ being the number of user factors. \n",
    "\n",
    "To create and train a latent factor model, we will use short recipes in TensorFlow. For those who are not familiar with this package, you may find useful to go over this [TensorFlow tutorial for beginners](https://www.tensorflow.org/tutorials/quickstart/beginner). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the number of factors we will use for item and user latent vectors. For a first trial, we will initialize it to 10, but you will be asked to inspect the impact of the number of factors on the performance later in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5OK5xB6m2UM"
   },
   "outputs": [],
   "source": [
    "no_factors = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow latent factor models\n",
    "\n",
    "The traditional definition of latent factor models is based on a shallow learning methodology. Basically, we create and initialize two matrices: a <code>user_matrix</code> and an <code>item_matrix</code>. Then, given a pair of <code>user_id</code> and <code>item_id</code>, we select the latent factors associated to that user and item from the two matrices, obtaining <code>user_vector</code> and <code>item_vector</code>. Finally, we compute the dot product between the vectors, namely the predicted rating (the ourpur of our model).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUASKPU0m5B0"
   },
   "outputs": [],
   "source": [
    "def create_shallow_model(no_factors, no_users, no_items):\n",
    "    # User branch\n",
    "    user_id = tf.keras.layers.Input(shape=[1], name='user_id')\n",
    "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
    "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
    "    # Item branch\n",
    "    item_id = tf.keras.layers.Input(shape=[1], name='item_id')\n",
    "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
    "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
    "    # Dot product \n",
    "    vectors_product = tf.keras.layers.dot([user_vector, item_vector], axes=1, normalize=False)\n",
    "    # Model definition\n",
    "    model = tf.keras.models.Model(inputs=[user_id, item_id], outputs=[vectors_product], name='shallow_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_shallow_model(no_factors, no_users, no_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually show our shallow latent factor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "Vgp1GwLRm62m",
    "outputId": "6f338ad6-91b8-466f-8167-6f577cacc59b"
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir='HB', dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the model.summary() to print a range of model characteristics, such as the number of trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zw9cGYtIm9p2",
    "outputId": "ab0d0f9c-3708-4984-edfb-2ccf26994b28"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you describe how the input data is transformed to the final output? \n",
    "- Why do the item_matrix and user_matrix layers return a vector of size (1, 10) each? \n",
    "- Why does the user_matrix layer have 604100 parameters? \n",
    "- Why does the item_matrix layer have 370700 parameters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep latent factor models (gift provided in this tutorial)\n",
    "\n",
    "In both the lecture and in the first part of this tutorial, we have seen that the rating is predicted by multiplying the two latent vectors. However, recent advances in the field have shown that it is possible to predict the rating by combining the two latent factors (e.g., with a concatenation) and making transformations to the combined vector in cascade with fully-connected layers (deep neural networks). In what follows, we can see an example of how a deep latent factor model can be defined. Please, pay attention to the concatenation and how the cascade of fully-connected layers is defined.\n",
    "\n",
    "Those who are interested in going into the details of this type of approaches may start by reading a milestone paper iin the field:\n",
    "\n",
    "- He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T. S. (2017). Neural Collaborative Filtering. In: WWW 2017 (pp. 173-182)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_model(no_factors, no_users, no_items):\n",
    "    # User branch\n",
    "    user_id = tf.keras.layers.Input(shape=[1], name='user_id')\n",
    "    user_matrix = tf.keras.layers.Embedding(no_users+1, no_factors, name='user_matrix')(user_id)\n",
    "    user_vector = tf.keras.layers.Flatten(name='user_vector')(user_matrix)\n",
    "    # Item branch\n",
    "    item_id = tf.keras.layers.Input(shape=[1], name='item_id')\n",
    "    item_matrix = tf.keras.layers.Embedding(no_items+1, no_factors, name='item_matrix')(item_id)\n",
    "    item_vector = tf.keras.layers.Flatten(name='item_vector')(item_matrix)\n",
    "    # Concantenation\n",
    "    vectors_concat = tf.keras.layers.Concatenate()([user_vector, item_vector])\n",
    "    vectors_concat_dropout = tf.keras.layers.Dropout(0.2)(vectors_concat)\n",
    "    # Backbone \n",
    "    dense_1 = tf.keras.layers.Dense(16,name='fc3')(vectors_concat_dropout)\n",
    "    dropout_1 = tf.keras.layers.Dropout(0.2,name='d3')(dense_1)\n",
    "    dense_2 = tf.keras.layers.Dense(8,name='fc4', activation='relu')(dropout_1)\n",
    "    dense_2_output = tf.keras.layers.Dense(1, activation='relu', name='activation')(dense_2)\n",
    "    # Model definition\n",
    "    model = tf.keras.models.Model(inputs=[user_id, item_id], outputs=[dense_2_output], name='deep_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_deep_model(no_factors, no_users, no_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir='HB', dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the summary, you can get an idea of the number of parameters to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To what extent do you think that a deep latent factor model is more powerful for this data and context?\n",
    "- How can we decide on the number and size of the fully-connected layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #4: Optimize the latent factor model\n",
    "---\n",
    "\n",
    "Once we define the architecture of our latent factor model, we can move on the optimization phase. In this part, we will focus on the shallow latent factor model, but you can easily inspect how the same analysis works for deep latent factor models later in the tutorial or after the tutorial. To start optimizing, we need to define the input data (pairs of user ids and item ids in the training set) and the ourput data (the ground-truth ratings). We initialize the model by capitalizing on the creation function defined previously. Then, we need to tell to the model how it will be optimized. This is done by the compile method. In this case, we adopt mean squared error, leaving experiments with other loss functions as a future work. Finally, we invoke the train method of the model, by passing the train data and labels, the number of epoches (number of times each training sample is seen by the model), and the size of the batches used during optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igO48-rWm-8E",
    "outputId": "5d7af225-08b6-4e7c-ff20-ec3af7a24b4c"
   },
   "outputs": [],
   "source": [
    "# Input-output data definition\n",
    "X_train = [train_ratings.user_id, train_ratings.item_id]\n",
    "y_train = train_ratings.rating\n",
    "\n",
    "# Model creation\n",
    "model = create_shallow_model(no_factors, no_users, no_items)\n",
    "\n",
    "# Model compiling \n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you reach this point, the latent factors are optimized, and you are ready to move to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #5: Predict the user-item ratings\n",
    "---\n",
    "\n",
    "The next step requires to make predictions on the user-item ratings. Specifically, we are interested in comparing the performance of the model in the training and test set, to assess the extent to which the model performance can generalize on unseen data. To make predictions, we can use the <code>predict</code> method of the model. This method takes pairs of user ids and item ids (similarly to the <code>fit</code> method) and returns the corresponding predicted ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions in the training set\n",
    "X_train = [train_ratings.user_id, train_ratings.item_id]\n",
    "y_train = train_ratings.rating\n",
    "y_train_pred = model.predict(X_train, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions in the test set\n",
    "X_test = [test_ratings.user_id, test_ratings.item_id]\n",
    "y_test = test_ratings.rating\n",
    "y_test_pred = model.predict(X_test, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred, y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #6: Evaluate a latent factor model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction accuracy metrics\n",
    "\n",
    "As seen in the last lecture, prediction accuracy metrics monitor the error rate in rating prediction. To this end, we need datasets with items rated by users (as ml-1m) and the historical user ratings constitute ground truth. The traditional prediction accuracy metrics are MAE Mean Absolute Error, MSE Mean Squared Error, RMSE Root Mean Squared Error (the most-widely used). In this tutorial, we show you how to compure the RMSE for the training and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('Train RMSE:', mean_squared_error(y_train.values, y_train_pred, squared=False))\n",
    "print('Test RMSE:', mean_squared_error(y_test.values, y_test_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision-support metrics\n",
    "\n",
    "Decision-support metrics measure how well a recommender helps users make good decisions. Good decisions are about choosing “relevant” items and avoiding “irrelevant” ones. The main decision-support metrics are P Precision and R Recall.\n",
    "\n",
    "- Precision is a measure of exactness and determines the fraction of relevant items retrieved out of all items retrieved, e.g., the proportion of recommended items actually relevant. Specifically, precision is about returning mostly relevant/useful items, to not waste user time. The assumption is that there is more relevant items than you want. \n",
    "\n",
    "- Recall is a measure of completeness and determines the fraction of relevant items retrieved out of all relevant items, e.g., the proportion of all relevant items recommended. Specifically, recall is about not missing relevant items,  i.e., not making a bad oversight. The assumption is that you have time to filter through results to find the items you need. \n",
    "\n",
    "Given that this definition covers the entire data set and does not target top-recommended items, it has been introduced the notion of cutoff k: the size of the recommended list. Specifically, Precision@k is the percentage of the top-k items that are “relevant”, and Recall@k is the percentage of the “relevant” items recommended in the top-k items. \n",
    "\n",
    "Please, refer to [Lecture 8 slides](https://moodle.epfl.ch/pluginfile.php/2903092/mod_resource/content/10/mlbeh_8_recsys_part2.pdf) for the formulas and the examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from prediction accuracy metrics, decision-support metrics require to compute the list recommended to the user. To this end, for each user, we identify the items already seen in the training phase and those who put apart for the test set. Then, we make the rating predictions for all items for that user, and we force a very low rating for items already seen in the training set (in such a way that they cannot be recommended anymore, the user might not want to get recommended a movie he or she has already seen). Finally, we sort the items based on decreasing predicted rating, and we take the top-k items. Based on this items, we are able to compute the main decision-support metrics, such as precision and recall.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o4cjO6vn4Op"
   },
   "outputs": [],
   "source": [
    "def predict_from_latent(model, uid, pids, train_ratings=None):\n",
    "    user_vector = model.get_layer('user_matrix').get_weights()[0][uid]\n",
    "    item_matrix = model.get_layer('item_matrix').get_weights()[0][pids]\n",
    "    scores = (np.dot(user_vector, item_matrix.T))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tk6vN2Z4n6z_"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10):\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "    precisions = []\n",
    "    # For each user\n",
    "    for user_id, user_test_rating in tqdm(test_ratings.groupby('user_id')):\n",
    "        # Retrieve already-seen items\n",
    "        train_pids = train_ratings[train_ratings['user_id'] == user_id]['item_id'].values\n",
    "        # Retrieve the unseen items\n",
    "        test_pids = set(user_test_rating['item_id'].values)\n",
    "        # Make rating predictions for all items for that user\n",
    "        predictions = pred_func(model, user_id, pid_array, train_ratings)\n",
    "        # Force a low rating to already-seen items\n",
    "        predictions[train_pids] = - math.inf\n",
    "        # Sort the items and het the top k\n",
    "        top_k = set(np.argsort(-predictions)[:k])\n",
    "        # Compute precision as per definition\n",
    "        precisions.append(len(top_k & test_pids) / float(k))\n",
    "    return precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS4k_SP8oCYJ",
    "outputId": "5f220fe4-6911-41d4-abcc-ae9827c6326a"
   },
   "outputs": [],
   "source": [
    "precisions = precision_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(precisions), np.std(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2.1\n",
    "\n",
    "In the following cell, we ask you to define a function that returns a list with the recalls for all users, individually. Specifically, the signature of the required function should be as follows: \n",
    "<code>recall_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10)</code>\n",
    "\n",
    "The returned value is a list of recalls, one per user, as similarly done for precision computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = recall_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(recalls), np.std(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-aware top-k metrics\n",
    "\n",
    "What we have seen so far (prediction accuracy and decision-support metrics) monitors how accurate the predictions are and how good a recommender system is at finding items. Now, when the recommender list the items, we are interested more in where are the relevant items are placed, and how good the recommender is at predicting relative preference between items. Rank-aware top-k metrics depend on the notion of relevance and take into account the position an item has been recommended. \n",
    "\n",
    "### Exercise #2.2\n",
    "In the following cell, we ask you to define a function that returns a list with the Mean Average Precisions (MAPs) for all users, individually. Specifically, the signature of the required function should be as follows: <code>map_at_k(model, pred_func, train_ratings, test_ratings, no_users, no_items, k=10)</code>. \n",
    "\n",
    "The returned value is a list of MAPs, one per user, as similarly done for precision and recall computation.\n",
    "\n",
    "Please, refer to [Lecture 8 slides](https://moodle.epfl.ch/pluginfile.php/2903092/mod_resource/content/10/mlbeh_8_recsys_part2.pdf) for the formulas and the examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = map_at_k(model, predict_from_latent, train_ratings, test_ratings, no_users, no_items, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(maps), np.std(maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3\n",
    "\n",
    "1. Train four shallow latent factor models on train_ratings, with 5, 25, 50, and 100 factors, respectively. Other parameters stay as per the previous examples. \n",
    "2. Compute the precisions for all users achieved by the four models in test_ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visually report the precisions by means of error bars. \n",
    "\n",
    "\n",
    "- What is the impact of the number of factors on the performance?\n",
    "- Do you have any argument on why these patterns arise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #4\n",
    "\n",
    "In the following cell, we ask you to define a <code>predict_from_pop</code> that returns a list of popularity scores, one per item (i.e., how many ratings that item has received in the data set). Specifically, we ask you to adhere to the following function signature: \n",
    "\n",
    "<code>predict_from_pop(model, uid, pids, train_ratings)</code>\n",
    "\n",
    "This function should return a list of size $|\\text{no_items}|$, where the cell at position $i$ represents the popularity of the item $i$ in the training data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #5\n",
    "\n",
    "- Train a shallow latent factor model on train_ratings, in the same way seen in the cells above.  \n",
    "- Create a grid with two plots, one for precision and one for recall. Each plot should include two error bars: the first one showing the scores achieved by the latent factor model and the second one showing the scores achieved by a recommender that suggests the same most popular items to all users (please use the predict_from_pop method appropriately). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #6\n",
    "\n",
    "- Create and provide an example usage of a function that, given a user id, prints the top k items (title included) recommended to that user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE CELL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further steps\n",
    "---\n",
    "\n",
    "- Repeat the analysis on another dataset. \n",
    "- Repeat on another evaluation methods (e.g., cross validation) and splitting strategies (e.g., fixed time stamp for all). \n",
    "- Repeat on another latent factor models, more advanced in terms of architecture. \n",
    "- Compare your model with traditional models manipulated by Surprise and seen in the previous lecture.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "\n",
    "In this tutorial, we introduced latent factor models, from their definition to their evaluation. Furthermore, we presented several evaluation methods and performance metrics, that are important for assessing the impact of the solution on the real world."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_04_01_Recommender_Systems_Case_Study",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
